{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "973152dd",
   "metadata": {},
   "source": [
    "# NotebookLM Clone: RAG Implementation with Gemini\n",
    "\n",
    "This notebook demonstrates how the RAG (Retrieval Augmented Generation) system works behind the scenes in our NotebookLM Clone application. We'll walk through the core components of the system step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c2c9f4",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "First, let's install and import all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0d41f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install google-generativeai PyPDF2 langchain faiss-cpu sentence-transformers numpy python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f280302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import PyPDF2\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Gemini API\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    print(\"Error: GEMINI_API_KEY not found in environment variables\")\n",
    "    print(\"Please create a .env file with your Gemini API key\")\n",
    "    sys.exit(1)\n",
    "    \n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "model = genai.GenerativeModel('gemini-1.5-pro')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1361c1d",
   "metadata": {},
   "source": [
    "## 2. Creating the RAG System\n",
    "\n",
    "Now let's implement our RAG system class that handles document processing and querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27858f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    def __init__(self):\n",
    "        # Initialize sentence transformer for embeddings\n",
    "        print(\"Loading embedding model...\")\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        \n",
    "        # Initialize text splitter\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,  # Characters per chunk\n",
    "            chunk_overlap=100,  # Overlap between chunks to maintain context\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]  # Priority order for splitting\n",
    "        )\n",
    "        \n",
    "        self.vector_store = None\n",
    "        self.documents = []\n",
    "        \n",
    "        print(\"RAG system initialized!\")\n",
    "    \n",
    "    def process_document(self, text, document_name=\"document\"):\n",
    "        print(f\"Processing document: {document_name}\")\n",
    "        \n",
    "        # Split text into chunks\n",
    "        print(\"Splitting text into chunks...\")\n",
    "        chunks = self.text_splitter.split_text(text)\n",
    "        print(f\"Created {len(chunks)} chunks\")\n",
    "        \n",
    "        # Store original documents for reference\n",
    "        self.documents.append({\n",
    "            \"name\": document_name,\n",
    "            \"text\": text,\n",
    "            \"chunks\": chunks\n",
    "        })\n",
    "        \n",
    "        # Create or update vector store\n",
    "        print(\"Creating vector embeddings...\")\n",
    "        if self.vector_store is None:\n",
    "            self.vector_store = FAISS.from_texts(chunks, self.embeddings)\n",
    "        else:\n",
    "            # Add new documents to existing vector store\n",
    "            self.vector_store.add_texts(chunks)\n",
    "            \n",
    "        print(\"Document processed successfully!\")\n",
    "        return len(chunks)\n",
    "    \n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        print(f\"Extracting text from PDF: {pdf_path}\")\n",
    "        text = \"\"\n",
    "        \n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                page_text = pdf_reader.pages[page_num].extract_text()\n",
    "                text += page_text + \"\\n\\n\"\n",
    "                \n",
    "        print(f\"Extracted {len(text)} characters from {len(pdf_reader.pages)} pages\")\n",
    "        return text\n",
    "    \n",
    "    def query(self, question, k=3):\n",
    "        if self.vector_store is None:\n",
    "            return \"Please process a document first.\"\n",
    "        \n",
    "        print(f\"Querying: {question}\")\n",
    "        \n",
    "        # Search for relevant documents\n",
    "        print(f\"Finding top {k} relevant chunks...\")\n",
    "        docs = self.vector_store.similarity_search(question, k=k)\n",
    "        \n",
    "        # Create context from relevant documents\n",
    "        context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        print(\"Generating response using Gemini...\")\n",
    "        # Generate response using Gemini\n",
    "        prompt = f\"\"\"\n",
    "        Answer the question based on the following context. If the answer is not in the context, just say that you don't know.\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        \n",
    "        response = model.generate_content(prompt)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response.text,\n",
    "            \"context\": context,\n",
    "            \"chunks_used\": [doc.page_content for doc in docs]\n",
    "        }\n",
    "\n",
    "\n",
    "# Create an instance of our RAG system\n",
    "rag = RAGSystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf42924",
   "metadata": {},
   "source": [
    "## 3. Sample Document Process & Query\n",
    "\n",
    "Let's process a sample text document and try to query it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a56a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text about artificial intelligence\n",
    "sample_text = \"\"\"\n",
    "# Introduction to Artificial Intelligence\n",
    "\n",
    "Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to natural intelligence displayed by humans or animals. Leading AI textbooks define the field as the study of \"intelligent agents\": any system that perceives its environment and takes actions that maximize its chance of achieving its goals.\n",
    "\n",
    "## History of AI\n",
    "\n",
    "The field of AI research was founded at a workshop held on the campus of Dartmouth College in the summer of 1956. The attendees, including John McCarthy, Marvin Minsky, Allen Newell, and Herbert Simon, became the founders and leaders of AI research. They and their students produced programs that were described as \"astonishing\": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems, and speaking English.\n",
    "\n",
    "By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense and laboratories had been established around the world. AI's founders were optimistic about the future: Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do.\"\n",
    "\n",
    "## AI Approaches\n",
    "\n",
    "### Machine Learning\n",
    "Machine learning (ML) is a subset of AI that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. ML focuses on the development of computer programs that can access data and use it to learn for themselves.\n",
    "\n",
    "Key machine learning algorithms include:\n",
    "1. Supervised learning (classification, regression)\n",
    "2. Unsupervised learning (clustering, dimensionality reduction)\n",
    "3. Reinforcement learning\n",
    "\n",
    "### Deep Learning\n",
    "Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised. Deep learning architectures such as deep neural networks, deep belief networks, recurrent neural networks, and convolutional neural networks have been applied to fields including computer vision, speech recognition, natural language processing, and more.\n",
    "\n",
    "## AI Applications\n",
    "\n",
    "### Healthcare\n",
    "AI in healthcare is used for tasks such as diagnosis of diseases, drug discovery, and personalized medicine. IBM's Watson for Oncology is trained to help doctors treat cancer patients by analyzing patient data and medical literature.\n",
    "\n",
    "### Finance\n",
    "AI is revolutionizing the finance industry through algorithmic trading, fraud detection, and customer service chatbots. JP Morgan's COIN program interprets commercial loan agreements in seconds, a task that previously took 360,000 hours of work annually by lawyers.\n",
    "\n",
    "### Transportation\n",
    "Self-driving vehicles use various AI techniques including computer vision and decision-making algorithms. Companies like Tesla, Waymo, and many traditional automakers are working on autonomous vehicles.\n",
    "\n",
    "## Ethical Considerations\n",
    "\n",
    "The development of AI raises ethical concerns related to privacy, security, and potential job displacement. Issues such as algorithmic bias, accountability, and the long-term impact of AI on human society are actively discussed by researchers, policymakers, and industry leaders.\n",
    "\n",
    "## Future of AI\n",
    "\n",
    "While narrow AI is focused on specific tasks, the long-term goal of many researchers is to create artificial general intelligence (AGI) - AI that can perform any intellectual task that a human can. However, experts have varying opinions on when, if ever, AGI will be achieved.\n",
    "\"\"\"\n",
    "\n",
    "# Process the sample text\n",
    "chunk_count = rag.process_document(sample_text, \"AI_Introduction\")\n",
    "print(f\"\\nDocument processed into {chunk_count} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0be824",
   "metadata": {},
   "source": [
    "Now let's ask some questions about the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8afd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question about the content\n",
    "question1 = \"When was AI research founded?\"\n",
    "result1 = rag.query(question1)\n",
    "\n",
    "print(\"\\nQuestion:\", question1)\n",
    "print(\"\\nAnswer:\")\n",
    "print(result1[\"answer\"])\n",
    "\n",
    "print(\"\\nRelevant chunks used:\")\n",
    "for i, chunk in enumerate(result1[\"chunks_used\"]):\n",
    "    print(f\"\\nChunk {i+1}:\\n{chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4147e8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask another question\n",
    "question2 = \"What are the key types of machine learning algorithms?\"\n",
    "result2 = rag.query(question2)\n",
    "\n",
    "print(\"\\nQuestion:\", question2)\n",
    "print(\"\\nAnswer:\")\n",
    "print(result2[\"answer\"])\n",
    "\n",
    "print(\"\\nRelevant chunks used:\")\n",
    "for i, chunk in enumerate(result2[\"chunks_used\"]):\n",
    "    print(f\"\\nChunk {i+1}:\\n{chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd59f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question that's not in the document\n",
    "question3 = \"What is the capital of France?\"\n",
    "result3 = rag.query(question3)\n",
    "\n",
    "print(\"\\nQuestion:\", question3)\n",
    "print(\"\\nAnswer:\")\n",
    "print(result3[\"answer\"])\n",
    "\n",
    "print(\"\\nRelevant chunks used:\")\n",
    "for i, chunk in enumerate(result3[\"chunks_used\"]):\n",
    "    print(f\"\\nChunk {i+1}:\\n{chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7e4891",
   "metadata": {},
   "source": [
    "## 4. Advanced: System Performance Analysis\n",
    "\n",
    "Let's analyze how chunk size and retrieval parameters affect performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5661927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_chunk_sizes(text, question, chunk_sizes=[500, 1000, 2000], overlaps=[50, 100, 200]):\n",
    "    results = {}\n",
    "    \n",
    "    for size in chunk_sizes:\n",
    "        for overlap in overlaps:\n",
    "            print(f\"\\nTesting chunk_size={size}, overlap={overlap}\")\n",
    "            \n",
    "            # Create new RAG system with specific parameters\n",
    "            test_rag = RAGSystem()\n",
    "            test_rag.text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=size,\n",
    "                chunk_overlap=overlap\n",
    "            )\n",
    "            \n",
    "            # Process document and query\n",
    "            num_chunks = test_rag.process_document(text, f\"chunk_{size}_{overlap}\")\n",
    "            result = test_rag.query(question)\n",
    "            \n",
    "            # Store results\n",
    "            results[f\"size_{size}_overlap_{overlap}\"] = {\n",
    "                \"num_chunks\": num_chunks,\n",
    "                \"answer\": result[\"answer\"],\n",
    "                \"chunks_used\": result[\"chunks_used\"]\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# We'll test this function with a specific question\n",
    "performance_q = \"What are the applications of AI in healthcare?\"\n",
    "# Uncomment to run the test (takes some time)\n",
    "# performance_results = test_chunk_sizes(sample_text, performance_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21af83a3",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "This notebook demonstrates the core RAG system used in our NotebookLM Clone application. The key components are:\n",
    "\n",
    "1. **Document processing**: Converting text into manageable chunks\n",
    "2. **Vector embeddings**: Creating numeric representations of text for semantic search\n",
    "3. **Similarity search**: Finding the most relevant chunks when a question is asked\n",
    "4. **Generative AI**: Using Gemini to generate answers based on the retrieved context\n",
    "\n",
    "The web application wraps this functionality in a user-friendly interface, allowing users to upload documents and interact with them through natural language questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
